{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi9f5TAkj3yxrtRAcmU1Yg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natarajankrishna/projects/blob/main/text_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhK_l2_bQtfk",
        "outputId": "90c2700b-5ec1-4134-96a1-0ec38b76c48b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=['There is a massive demand for low-weight high strength materials in automotive, space aerospace, and even structural industries in this present engineering world. These industries attract composites only because of their high strength, resistance to wear, and low weight. Among these composites, metal matrix composite finds wide applications due to its elevated properties, excellent resistance property, corrosion resistance, etc. The reinforcements exist in particles, fiber, and whiskers. Among the three, particles play an important role because of their availability and wettability with the metal matrix. Additionally, among the various metal matrices such as aluminum, magnesium, copper, titanium, etc., aluminum plays a vital role among metal matrices because of its cost, availability in abundance, and castability. Stir casting is the most inexpensive and straightforward composite fabrication technique among the prevailing techniques. Even though so many factors contribute to the elevated property of composites, metal matrix, and reinforcement phase, uniform distribution and wettability are essential factors among all the other factors. This review aims to develop a composite with elevated property in a cost-effective manner. Cost includes metal matrix, reinforcement, and processing technique. Various works have been tabulated to achieve the above objective, and analysis was carried out on tensile strength concerning microstructure. This review paper explores the challenges in composite fabrication and finds a solution to overcome them']"
      ],
      "metadata": {
        "id": "d5jxvZXhQ6cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = []\n",
        "for s in ['text']:\n",
        "    sentences.append(sent_tokenize(s))\n",
        "\n",
        "sentences = [y for x in sentences for y in x]"
      ],
      "metadata": {
        "id": "FNXh4GKJRtjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = {}\n",
        "f = 'text'\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "\n",
        "\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "clean_sentences = [s.lower() for s in clean_sentences]\n",
        "stop_words = stopwords.words('english')\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZCsJCGRSasV",
        "outputId": "f1a675f1-2847-4130-d7d1-68b9c350201c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-0f05b036608e>:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "  sentence_vectors.append(v)"
      ],
      "metadata": {
        "id": "qA88VQL_S0dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
      ],
      "metadata": {
        "id": "NnsBLIIzTGV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy networkx\n",
        "import networkx as nx\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg5ExYcCVvVI",
        "outputId": "7fac4b94-1f45-412a-9945-d505ec3687d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (3.0)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = nx.pagerank(nx_graph)\n",
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "for i in range(5):\n",
        "  print(\"ARTICLE:\")\n",
        "  print(['text'][i])\n",
        "  print('\\n')\n",
        "  print(\"SUMMARY:\")\n",
        "  print(ranked_sentences[i][1])\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "1Vap_HFXbEr5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}