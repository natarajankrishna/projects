{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5SYHcMFqKGijGXeAc8JsK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natarajankrishna/natarajan/blob/main/image_caption_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0pXjk--WUc-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import string\n",
        "import os\n",
        "import glob\n",
        "from PIL import Image\n",
        "from time import time\n",
        "\n",
        "from keras import Input, layers\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_path = \"../input/flickr8k/Data/Flickr8k_text/Flickr8k.token.txt\"\n",
        "train_images_path = '../input/flickr8k/Data/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "test_images_path = '../input/flickr8k/Data/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "images_path = '../input/flickr8k/Data/Flicker8k_Dataset/'\n",
        "glove_path = '../input/glove6b'\n",
        "\n",
        "doc = open(token_path,'r').read()\n",
        "print(doc[:410])"
      ],
      "metadata": {
        "id": "8yOuPjc-WrBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descriptions = dict()\n",
        "for line in doc.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        if len(line) > 2:\n",
        "          image_id = tokens[0].split('.')[0]\n",
        "          image_desc = ' '.join(tokens[1:])\n",
        "          if image_id not in descriptions:\n",
        "              descriptions[image_id] = list()\n",
        "          descriptions[image_id].append(image_desc)"
      ],
      "metadata": {
        "id": "L_3-JXKOW2UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = str.maketrans('', '', string.punctuation)\n",
        "for key, desc_list in descriptions.items():\n",
        "    for i in range(len(desc_list)):\n",
        "        desc = desc_list[i]\n",
        "        desc = desc.split()\n",
        "        desc = [word.lower() for word in desc]\n",
        "        desc = [w.translate(table) for w in desc]\n",
        "        desc_list[i] =  ' '.join(desc)"
      ],
      "metadata": {
        "id": "wmD4LcbYW6eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set()\n",
        "for key in descriptions.keys():\n",
        "        [vocabulary.update(d.split()) for d in descriptions[key]]\n",
        "print('Original Vocabulary Size: %d' % len(vocabulary))"
      ],
      "metadata": {
        "id": "1f1wDGTZW-Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = list()\n",
        "for key, desc_list in descriptions.items():\n",
        "    for desc in desc_list:\n",
        "        lines.append(key + ' ' + desc)\n",
        "new_descriptions = '\\n'.join(lines)"
      ],
      "metadata": {
        "id": "Bbk-3aG5XDj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = open(train_images_path,'r').read()\n",
        "dataset = list()\n",
        "for line in doc.split('\\n'):\n",
        "    if len(line) > 1:\n",
        "      identifier = line.split('.')[0]\n",
        "      dataset.append(identifier)\n",
        "\n",
        "train = set(dataset)"
      ],
      "metadata": {
        "id": "InkDaveKXICz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = glob.glob(images_path + '*.jpg')\n",
        "train_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\n",
        "train_img = []\n",
        "for i in img: \n",
        "    if i[len(images_path):] in train_images:\n",
        "        train_img.append(i)\n",
        "\n",
        "test_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\n",
        "test_img = []\n",
        "for i in img: \n",
        "    if i[len(images_path):] in test_images: \n",
        "        test_img.append(i)"
      ],
      "metadata": {
        "id": "fZgNda2kXMyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_descriptions = dict()\n",
        "for line in new_descriptions.split('\\n'):\n",
        "    tokens = line.split()\n",
        "    image_id, image_desc = tokens[0], tokens[1:]\n",
        "    if image_id in train:\n",
        "        if image_id not in train_descriptions:\n",
        "            train_descriptions[image_id] = list()\n",
        "        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "        train_descriptions[image_id].append(desc)"
      ],
      "metadata": {
        "id": "C0VWT99lXRGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_captions = []\n",
        "for key, val in train_descriptions.items():\n",
        "    for cap in val:\n",
        "        all_train_captions.append(cap)"
      ],
      "metadata": {
        "id": "Gd_Y4bgKXWKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in all_train_captions:\n",
        "    nsents += 1\n",
        "    for w in sent.split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "\n",
        "print('Vocabulary = %d' % (len(vocab)))"
      ],
      "metadata": {
        "id": "dV9Nrle3XaTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoix[w] = ix\n",
        "    ixtoword[ix] = w\n",
        "    ix += 1\n",
        "\n",
        "vocab_size = len(ixtoword) + 1"
      ],
      "metadata": {
        "id": "uOzdeU9eXh-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_desc = list()\n",
        "for key in train_descriptions.keys():\n",
        "    [all_desc.append(d) for d in train_descriptions[key]]\n",
        "lines = all_desc\n",
        "max_length = max(len(d.split()) for d in lines)\n",
        "\n",
        "print('Description Length: %d' % max_length)"
      ],
      "metadata": {
        "id": "IZddreYKXiy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {} \n",
        "f = open(os.path.join(glove_path, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "ueLataz4Xmmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 200\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in wordtoix.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "ae5Che26XrHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = InceptionV3(weights='imagenet')\n",
        "model_new = Model(model.input, model.layers[-2].output)"
      ],
      "metadata": {
        "id": "WezIPPNAXugv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image_path):\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "2VRYvVmdX2yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(image):\n",
        "    image = preprocess(image) \n",
        "    fea_vec = model_new.predict(image) \n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
        "    return fea_vec\n",
        "\n",
        "encoding_train = {}\n",
        "for img in train_img:\n",
        "    encoding_train[img[len(images_path):]] = encode(img)\n",
        "train_features = encoding_train\n",
        "\n",
        "encoding_test = {}\n",
        "for img in test_img:\n",
        "    encoding_test[img[len(images_path):]] = encode(img)"
      ],
      "metadata": {
        "id": "a4ZolHY9X76R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "755XIj06YAS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "29EfArVIYG4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            n+=1\n",
        "            # retrieve the photo feature\n",
        "            photo = photos[key+'.jpg']\n",
        "            for desc in desc_list:\n",
        "                # encode the sequence\n",
        "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "                # split one sequence into multiple X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pair\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    # store\n",
        "                    X1.append(photo)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "\n",
        "      if n==num_photos_per_batch:\n",
        "                yield ([array(X1), array(X2)], array(y))\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "metadata": {
        "id": "0PYCKJjYYQD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "batch_size = 3\n",
        "steps = len(train_descriptions)//batch_size\n",
        "\n",
        "generator = data_generator(train_descriptions, train_features, wordtoix, max_length, batch_size)\n",
        "model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)"
      ],
      "metadata": {
        "id": "DOQ5jq7DYVib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedySearch(photo):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ],
      "metadata": {
        "id": "lxHpa8UyYqkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_predictions(image, beam_index = 3):\n",
        "    start = [wordtoix[\"startseq\"]]\n",
        "    start_word = [[start, 0.0]]\n",
        "    while len(start_word[0][0]) < max_length:\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
        "            preds = model.predict([image,par_caps], verbose=0)\n",
        "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "            # Getting the top <beam_index>(n) predictions and creating a \n",
        "            # new list so as to put them via the model again\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "                    \n",
        "        start_word = temp\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        start_word = start_word[-beam_index:]\n",
        "    \n",
        "    start_word = start_word[-1][0]\n",
        "    intermediate_caption = [ixtoword[i] for i in start_word]\n",
        "    final_caption = []\n",
        "    \n",
        "    for i in intermediate_caption:\n",
        "        if i != 'endseq':\n",
        "            final_caption.append(i)\n",
        "        else:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "eEyCoXV-YwAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pic = '2398605966_1d0c9e6a20.jpg'\n",
        "image = encoding_test[pic].reshape((1,2048))\n",
        "x=plt.imread(images_path+pic)\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "\n",
        "print(\"Greedy Search:\",greedySearch(image))\n",
        "print(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\n",
        "print(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\n",
        "print(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\n",
        "print(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))"
      ],
      "metadata": {
        "id": "rwwpxUk0ZDla"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}